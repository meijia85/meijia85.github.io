# 深度学习简介
## 机器学习
### 机器学习的目标是什么
&emsp;&emsp;&emsp;&emsp;用通俗的语言讲就是，从给定的信息中抽取另一个层次的信息，学习的过程可以是有监督的也可以是无监督的，输入的信息可能是图像、语言、文本、行为特征的各种各样的数据，输出可能是分类、回归、聚类、特征。  
&emsp;&emsp;&emsp;&emsp;用数学的语言讲就是，给定信息X，求一个函数f，满足\\(f(X)=Y\\)。例如Logistic回归的分类函数$$Y=\frac{1}{1+e^{-W^TX}}$$和线性[SVM](https://www.csie.ntu.edu.tw/~cjlin/libsvmvm/)的分类函数$$Y=W^TX+b$$还有概率模型、树模型等都可以理解成一个函数。
### 机器学习是在干什么
&emsp;&emsp;&emsp;&emsp;选定一种分类函数之后还需要定义个函数用于度量学习结果的好坏，最直观的是使用预测结果与标注结果的均方误差，但是通常会根据目标函数的含义选择一个更优的。例如Logistic回归是极大化所有样本的对数似然函数（使样本属于对应类目的概率最大），[SVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)是最大化分类间隔（使支持向量所在的超平面间隔最大）。求解过程通常是，如果是最大化问题先转换成最小化问题，使用梯度下降迭代寻找最优解。需要最小化的函数就是损失函数或者代价函数，在深度学习中，网络的最后通常都需要定义Loss Fuction。  
&emsp;&emsp;&emsp;&emsp;除了优化分类函数结果外，通常还会对分类函数的参数做约束（即正则项）。添加正则项的目的是为了缩小参数空间，防止过拟合。正则项通常有L1和L2正则，L1使得尽可能多的参数为0，L2使得参数的绝对值尽可能小。
<!--more-->
## 深度学习
### 神经网络
&emsp;&emsp;&emsp;&emsp;神经网络包含输入层（最左边）、输出层（最右边）、一到多个隐含层（中间），结构如图：  
![网络结构](http://neuralnetworksanddeeplearning.com/images/tikz11.png)  
每一层都可以有一到多个节点，除输入层外的每一个节点都是一个神经元，神经元的输入是上一层的输出X，神经元的输出是\\(g(WX+b)\\)，如图：  
![神经元](http://neuralnetworksanddeeplearning.com/images/tikz0.png)  
g是一个非线性函数，通常使用sigmoid或tanh函数，sigmoid函数如图：  
![Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)  
神经网络的训练采用[反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95) 。  
&emsp;&emsp;&emsp;&emsp;神经网络有强大的描述能力，只要有足够的神经元和层次神经网络能够[拟合任意函数](http://neuralnetworksanddeeplearning.com/chap4.html)，但是神经网络的训练需要大量的样本和计算资源。
### 深度学习
&emsp;&emsp;&emsp;&emsp;当更多的标注数据和更强的计算能力成为可能时，更大更深的神经网络又重新走入了人们的视线。不仅仅是神经网络，周志华老师提出的[Deep forest](https://github.com/kingfengji/gcForest)证明了更深更大的树模型同样很有效；但是神经网络的计算更加适合GPU并行加速，所以通常选用神经网络做深度学习。为什么叫深度学习？因为这种网络结构通常越深描述能力越强，何凯明提出的[ResNet](https://github.com/KaimingHe/deep-residual-networks)已经深达一千多层。
### 常见的网络模块
&emsp;&emsp;&emsp;&emsp;神经网络是分模块的，每个模块都有不同意义，利用这些模块可以方便的组建各种各样的网络模型。先看一下[GoogLeNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet)的网络结构：![GoogLeNet](http://www-gageet-com-static.smartgslb.com/wp-content/uploads/2014/09/googlenet.jpg)  
下面具体介绍一些常见的网络结构：
#### FullConnected
&emsp;&emsp;&emsp;&emsp;FullConnected，从名字上就能猜到含义，全连接，输出节点与每一个输入节点都相连，即任一输入节点对每个输出节点响应都有贡献。全连接层的缺陷是，参数规模太大训练困难，对于输入M个节点输出N个节点的全连接有\\((M+1)N\\)个参数。当M和N等于1000时，参数就超过百万，对于图像而言输入1000只是个32x32的小缩略图。  
&emsp;&emsp;&emsp;&emsp;全连接层需要设置输出节点的数量。
#### Convolution
&emsp;&emsp;&emsp;&emsp;Convolution卷积，卷积就是用一个窗口（卷积核）在输入上面左右上下滑动求加权和加偏置，如图：![Convolution](http://deeplearning.stanford.edu/wiki/images/6/6c/Convolution_schematic.gif)  
卷积层利用局部感受野和权值共享大大减少了参数的数量，局部感受野的含义是指在某个位置抽取特征的时候只需要关注临近的小范围输入信息（比如看一个人的照片时只用包含眼睛周围一小部分像素就能认出眼睛，听一段话时只需要一个片段就能听出“好”字而不需要从头到尾听完整段话），权值共享的含义是指特征在输入的任何位置都可以用相同的模式（权值、参数）提取，然后使用多个卷积核提取多种多样的特征。  
&emsp;&emsp;&emsp;&emsp;卷积层需要设置卷积核大小和卷积核数量，卷积核大小通常为奇数，除此之外还要考虑卷积核滑动时的步长，以及边界的处理方式。在边界时，窗口大于1的卷积核会有部分超出输入范围，此时是选择丢弃还是填充，丢弃会导致输出大小小于输入。
#### Activation
&emsp;&emsp;&emsp;&emsp;Activation激活函数或响应函数，比如前面在神经网络中讲到的非线性函数sigmoid和tanh。非线性的激活函数是必须的，因为多层的线性变换组合还是线性变换，没有非线性变换多层就没有意义。在深层的神经网络中使用sigmoid和tanh会存在梯度消失的问题，如图![sigmoid梯度](http://whiteboard.ping.se/uploads/MachineLearning/sigmoid700.png)  
sigmoid和tanh的梯度在很大的范围里都是接近于0，sigmoid梯度最大值才0.25，tanh的梯度最大值才1，在梯度[反向传导](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)到网络底层时已经变得非常小，导致网络更新缓慢训练困难。  
&emsp;&emsp;&emsp;&emsp;ReLU（rectified linear unit修正线性单元）及其变体是在深度学习中常用的激活函数，ReLU的定义\\(f(x)=max(0,x)\\)，如图![ReLU](https://i.stack.imgur.com/A201o.png)  
ReLU在输出大于0时梯度永远为1，解决了梯度消失的问题，ReLU在输出小于0时置零可以看做在提供非线性的同时做特征筛选；Leaky ReLU在输出小于0时也给一个小的权重，解决ReLU节点在输出小于0后无法再训练调整的问题。
#### Pooling
&emsp;&emsp;&emsp;&emsp;Pooling池化，通常有Max Pooling和Average Pooling两种，Pooling是对输入做分块，Max Pooling取块内最大值，Average Pooling取块内平均值。如图![Pooling](https://i.stack.imgur.com/bhXRN.png)  
&emsp;&emsp;&emsp;&emsp;Pooling需要设置块大小K和块采样间隔S，采样间隔决定了输出大小，一维的情况下假如输入大小M，输出大小\\(\frac{M-K}{S}+1\\)（除法取整方法，caffe中取ceil，mxnet中默认参数valid取floor，full参数取ceil）。  
&emsp;&emsp;&emsp;&emsp;关于Convolution和Pooling的kernel_size核大小、stride步长、padding填充需要仔细设置，否则网络输出的大小会不符合预期。像GooLeNet和ResNet输入图大小都是224x224，第一个卷积层卷积核7x7步长2x2，中间使用了4个2x2的Max Pooling，最后使用一个7x7的Average Pooling，所有卷积都做了完整Padding，所以输入图大小224x224（2x(2x2x2x2)x7）。
#### Dropout
&emsp;&emsp;&emsp;&emsp;Dropout在训练的时候随机的丢弃一部分特征，学习类似于人在少量信息不完整时的预测能力提高模型泛化性能；也有ensemble学习的意义，类似于随机森林随机的挑选一部分特征训练树，在预测的时候综合所有树的效果。Dropout效果如图：
![Dropout](http://neuralnetworksanddeeplearning.com/images/tikz31.png)  
&emsp;&emsp;&emsp;&emsp;Dropout只是在训练阶段会有随机丢弃，预测时并没有丢弃；Dropout在每次迭代时都会随机丢弃，所以Dropout层每次迭代时选择的特征都可能不同。
### 常见的损失函数
&emsp;&emsp;&emsp;&emsp;下面介绍一些常见的损失函数，LogisticRegression、Softmax和SVM/HingeLoss用于分类问题，LinearRegression和MAERegression用于回归问题，ContrastiveLoss和TripletLoss在分类的同时最小化类内距离最大化类间距离。需要根据任务目标选择或设计合适的loss，一个神经网络可以定义多个loss（多任务学习）。多任务的时候需要将各个loss限制在一个量级，在自定义loss时也需要注意loss的范围输出不要太大。
#### LogisticRegression
&emsp;&emsp;&emsp;&emsp;LogisticRegression逻辑斯蒂回归，用于二类分类或者概率预测，LogisticRegression先经过Sigmoid函数把输出约束到\\((0,1)\\)可以看做正类的概率，用输出的概率值计算交叉熵损失。  
&emsp;&emsp;&emsp;&emsp;Loss计算公式：$$CE(y, \hat{y} ) = -y\log(\hat{y})-(1-y)\log(1-\hat{y})$$
#### Softmax
&emsp;&emsp;&emsp;&emsp;Softmax用于多类问题，是多项Logistic回归，两类分类时等价于LogisticRegression，Softmax公式输出可以看做每一类对应的概率，根据Softmax公式输出结果计算交叉熵损失。  
&emsp;&emsp;&emsp;&emsp;Softmax计算公式：$$softmax(y)_i = \frac{exp(y_i)}{\sum_j exp(y_j)}$$  
&emsp;&emsp;&emsp;&emsp;Loss计算公式：$$CE(label, output) = - \sum_i label_i \log(output_i)$$
#### SVM/HingeLoss
&emsp;&emsp;&emsp;&emsp;在mxnet中叫SVM损失，在Caffe里叫HingeLoss，等同于svm中最大化分类间隔。神经网络里仅有一个全连接层，再加上HingeLoss，等同于普通线性SVM分类器。  
&emsp;&emsp;&emsp;&emsp;[Loss计算公式](http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1HingeLossLayer.html)：$$ E = \frac{1}{N} \sum\limits_{n=1}^N \sum\limits_{k=1}^K [\max(0, 1 - \delta(l_n==k)t_{nk})] ^ p $$ $$ \delta(condition) = 1 \mbox{if condition; else} -1 $$
#### LinearRegression
&emsp;&emsp;&emsp;&emsp;LinearRegression线性回归，用于回归问题，输出是预测值。  
&emsp;&emsp;&emsp;&emsp;Loss计算公式：$$SquaredLoss(y, \hat{y} ) = \frac{1}{n} \sum_{i=0}^{n-1} \left( y_i - \hat{y}_i \right)^2$$
#### MAERegression
&emsp;&emsp;&emsp;&emsp;MAERegression线性回归，用于回归问题，输出是预测值。  
&emsp;&emsp;&emsp;&emsp;Loss计算公式：$$MAE(y, \hat{y} ) = \frac{1}{n} \sum_{i=0}^{n-1} \left| y_i - \hat{y}_i \right|$$
#### ContrastiveLoss
&emsp;&emsp;&emsp;&emsp;ContrastiveLoss对比损失，要求输入有两路a和b，目的是要求当a和b是同一类时距离最小化，不是同一类时距离最大化。  
&emsp;&emsp;&emsp;&emsp;[Loss计算公式](http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1ContrastiveLossLayer.html)：$$ E = \frac{1}{2N} \sum\limits_{n=1}^N \left(y\right) d^2 + \left(1-y\right) \max \left(margin-d, 0\right)^2 $$ $$ d = \left| \left| a_n - b_n \right| \right|_2 $$
#### TripletLoss
&emsp;&emsp;&emsp;&emsp;TripletLoss比ContrastiveLoss约束更加严格，要求三路输入a、p、n，ap是同一类，an不是同一类，目标也是要求同一类距离最小化、不同类距离最大化。TripletLoss在Google的FaceNet提出，用于人脸验证时人脸特征对比。  
&emsp;&emsp;&emsp;&emsp;[Loss计算公式](https://arxiv.org/pdf/1503.03832v1.pdf)$$loss({a, p, n}) = \frac{1}{N} \sum \max(0, ||a_i - p_i||^2 + \alpha - ||a_i - n_i||^2)$$
## 动手试试
#### [mxnet中手写数字识别的例子](https://mxnet.incubator.apache.org/tutorials/python/mnist.html)  
#### [caffe中手写数字识别的例子](https://github.com/BVLC/caffe/tree/master/examples/mnist)
### 训练流程
&emsp;&emsp;&emsp;&emsp;准备数据集，训练和测试集一定要分好；深度学习容易过拟合，没有合理划分训练测试集不能真实反应模型的效果。  
&emsp;&emsp;&emsp;&emsp;定义模型，模型怎么定义没有头绪先试试知名的开源模型（[ResNet](https://github.com/KaimingHe/deep-residual-networks)、[GoogLeNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet)等），至少开源模型经过验证模型不会出错；自定义层时，卷积层和全连接后面一定要有非线性激活函数（ReLU是首选）；最后一层的全连接用于分类时不需要再加激活函数，一般Loss层里有；根据输入大小仔细设计kernel_size、stride和padding。  
&emsp;&emsp;&emsp;&emsp;模型初始化，如果用开源模型可以在训练好的参数上做finetune，收敛速度会快很多；自定义层用xavier初始化一般不会错。  
&emsp;&emsp;&emsp;&emsp;迭代训练，需要在学习过程中减少学习率，观察训练集和测试集loss的变化。  
&emsp;&emsp;&emsp;&emsp;训练结束或提前终止，观察测试集的loss，如果好久没有下降可以提前结束训练了。  

## 引用
[SVM](https://www.csie.ntu.edu.tw/~cjlin/libsvm/)  
[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/index.html)  
[反向传导算法](http://deeplearning.stanford.edu/wiki/index.php/%E5%8F%8D%E5%90%91%E4%BC%A0%E5%AF%BC%E7%AE%97%E6%B3%95)  
[mxnet](https://mxnet.incubator.apache.org/api/python/symbol.html#neural-network)  
[caffe](http://caffe.berkeleyvision.org/tutorial/layers.html)  
[Deep forest](https://github.com/kingfengji/gcForest)  
[GoogLeNet](https://github.com/BVLC/caffe/tree/master/models/bvlc_googlenet)  
[ResNet](https://github.com/KaimingHe/deep-residual-networks)  

作者: 梅佳  
写于：2017年09月
